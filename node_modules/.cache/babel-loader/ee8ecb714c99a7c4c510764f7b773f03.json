{"ast":null,"code":"/**\n * mux.js\n *\n * Copyright (c) Brightcove\n * Licensed Apache-2.0 https://github.com/videojs/mux.js/blob/master/LICENSE\n *\n * A stream-based mp2t to mp4 converter. This utility can be used to\n * deliver mp4s to a SourceBuffer on platforms that support native\n * Media Source Extensions.\n */\n'use strict';\n\nvar Stream = require('../utils/stream.js');\n\nvar mp4 = require('./mp4-generator.js');\n\nvar frameUtils = require('./frame-utils');\n\nvar audioFrameUtils = require('./audio-frame-utils');\n\nvar trackDecodeInfo = require('./track-decode-info');\n\nvar m2ts = require('../m2ts/m2ts.js');\n\nvar AdtsStream = require('../codecs/adts.js');\n\nvar H264Stream = require('../codecs/h264').H264Stream;\n\nvar AacStream = require('../aac');\n\nvar isLikelyAacData = require('../aac/utils').isLikelyAacData; // constants\n\n\nvar AUDIO_PROPERTIES = ['audioobjecttype', 'channelcount', 'samplerate', 'samplingfrequencyindex', 'samplesize'];\nvar VIDEO_PROPERTIES = ['width', 'height', 'profileIdc', 'levelIdc', 'profileCompatibility']; // object types\n\nvar _VideoSegmentStream, _AudioSegmentStream, _Transmuxer, _CoalesceStream;\n/**\n * Compare two arrays (even typed) for same-ness\n */\n\n\nvar arrayEquals = function arrayEquals(a, b) {\n  var i;\n\n  if (a.length !== b.length) {\n    return false;\n  } // compare the value of each element in the array\n\n\n  for (i = 0; i < a.length; i++) {\n    if (a[i] !== b[i]) {\n      return false;\n    }\n  }\n\n  return true;\n};\n\nvar generateVideoSegmentTimingInfo = function generateVideoSegmentTimingInfo(baseMediaDecodeTime, startDts, startPts, endDts, endPts, prependedContentDuration) {\n  var ptsOffsetFromDts = startPts - startDts,\n      decodeDuration = endDts - startDts,\n      presentationDuration = endPts - startPts; // The PTS and DTS values are based on the actual stream times from the segment,\n  // however, the player time values will reflect a start from the baseMediaDecodeTime.\n  // In order to provide relevant values for the player times, base timing info on the\n  // baseMediaDecodeTime and the DTS and PTS durations of the segment.\n\n  return {\n    start: {\n      dts: baseMediaDecodeTime,\n      pts: baseMediaDecodeTime + ptsOffsetFromDts\n    },\n    end: {\n      dts: baseMediaDecodeTime + decodeDuration,\n      pts: baseMediaDecodeTime + presentationDuration\n    },\n    prependedContentDuration: prependedContentDuration,\n    baseMediaDecodeTime: baseMediaDecodeTime\n  };\n};\n/**\n * Constructs a single-track, ISO BMFF media segment from AAC data\n * events. The output of this stream can be fed to a SourceBuffer\n * configured with a suitable initialization segment.\n * @param track {object} track metadata configuration\n * @param options {object} transmuxer options object\n * @param options.keepOriginalTimestamps {boolean} If true, keep the timestamps\n *        in the source; false to adjust the first segment to start at 0.\n */\n\n\n_AudioSegmentStream = function AudioSegmentStream(track, options) {\n  var adtsFrames = [],\n      sequenceNumber = 0,\n      earliestAllowedDts = 0,\n      audioAppendStartTs = 0,\n      videoBaseMediaDecodeTime = Infinity;\n  options = options || {};\n\n  _AudioSegmentStream.prototype.init.call(this);\n\n  this.push = function (data) {\n    trackDecodeInfo.collectDtsInfo(track, data);\n\n    if (track) {\n      AUDIO_PROPERTIES.forEach(function (prop) {\n        track[prop] = data[prop];\n      });\n    } // buffer audio data until end() is called\n\n\n    adtsFrames.push(data);\n  };\n\n  this.setEarliestDts = function (earliestDts) {\n    earliestAllowedDts = earliestDts - track.timelineStartInfo.baseMediaDecodeTime;\n  };\n\n  this.setVideoBaseMediaDecodeTime = function (baseMediaDecodeTime) {\n    videoBaseMediaDecodeTime = baseMediaDecodeTime;\n  };\n\n  this.setAudioAppendStart = function (timestamp) {\n    audioAppendStartTs = timestamp;\n  };\n\n  this.flush = function () {\n    var frames, moof, mdat, boxes; // return early if no audio data has been observed\n\n    if (adtsFrames.length === 0) {\n      this.trigger('done', 'AudioSegmentStream');\n      return;\n    }\n\n    frames = audioFrameUtils.trimAdtsFramesByEarliestDts(adtsFrames, track, earliestAllowedDts);\n    track.baseMediaDecodeTime = trackDecodeInfo.calculateTrackBaseMediaDecodeTime(track, options.keepOriginalTimestamps);\n    audioFrameUtils.prefixWithSilence(track, frames, audioAppendStartTs, videoBaseMediaDecodeTime); // we have to build the index from byte locations to\n    // samples (that is, adts frames) in the audio data\n\n    track.samples = audioFrameUtils.generateSampleTable(frames); // concatenate the audio data to constuct the mdat\n\n    mdat = mp4.mdat(audioFrameUtils.concatenateFrameData(frames));\n    adtsFrames = [];\n    moof = mp4.moof(sequenceNumber, [track]);\n    boxes = new Uint8Array(moof.byteLength + mdat.byteLength); // bump the sequence number for next time\n\n    sequenceNumber++;\n    boxes.set(moof);\n    boxes.set(mdat, moof.byteLength);\n    trackDecodeInfo.clearDtsInfo(track);\n    this.trigger('data', {\n      track: track,\n      boxes: boxes\n    });\n    this.trigger('done', 'AudioSegmentStream');\n  };\n};\n\n_AudioSegmentStream.prototype = new Stream();\n/**\n * Constructs a single-track, ISO BMFF media segment from H264 data\n * events. The output of this stream can be fed to a SourceBuffer\n * configured with a suitable initialization segment.\n * @param track {object} track metadata configuration\n * @param options {object} transmuxer options object\n * @param options.alignGopsAtEnd {boolean} If true, start from the end of the\n *        gopsToAlignWith list when attempting to align gop pts\n * @param options.keepOriginalTimestamps {boolean} If true, keep the timestamps\n *        in the source; false to adjust the first segment to start at 0.\n */\n\n_VideoSegmentStream = function VideoSegmentStream(track, options) {\n  var sequenceNumber = 0,\n      nalUnits = [],\n      gopsToAlignWith = [],\n      config,\n      pps;\n  options = options || {};\n\n  _VideoSegmentStream.prototype.init.call(this);\n\n  delete track.minPTS;\n  this.gopCache_ = [];\n  /**\n    * Constructs a ISO BMFF segment given H264 nalUnits\n    * @param {Object} nalUnit A data event representing a nalUnit\n    * @param {String} nalUnit.nalUnitType\n    * @param {Object} nalUnit.config Properties for a mp4 track\n    * @param {Uint8Array} nalUnit.data The nalUnit bytes\n    * @see lib/codecs/h264.js\n   **/\n\n  this.push = function (nalUnit) {\n    trackDecodeInfo.collectDtsInfo(track, nalUnit); // record the track config\n\n    if (nalUnit.nalUnitType === 'seq_parameter_set_rbsp' && !config) {\n      config = nalUnit.config;\n      track.sps = [nalUnit.data];\n      VIDEO_PROPERTIES.forEach(function (prop) {\n        track[prop] = config[prop];\n      }, this);\n    }\n\n    if (nalUnit.nalUnitType === 'pic_parameter_set_rbsp' && !pps) {\n      pps = nalUnit.data;\n      track.pps = [nalUnit.data];\n    } // buffer video until flush() is called\n\n\n    nalUnits.push(nalUnit);\n  };\n  /**\n    * Pass constructed ISO BMFF track and boxes on to the\n    * next stream in the pipeline\n   **/\n\n\n  this.flush = function () {\n    var frames,\n        gopForFusion,\n        gops,\n        moof,\n        mdat,\n        boxes,\n        prependedContentDuration = 0,\n        firstGop,\n        lastGop; // Throw away nalUnits at the start of the byte stream until\n    // we find the first AUD\n\n    while (nalUnits.length) {\n      if (nalUnits[0].nalUnitType === 'access_unit_delimiter_rbsp') {\n        break;\n      }\n\n      nalUnits.shift();\n    } // Return early if no video data has been observed\n\n\n    if (nalUnits.length === 0) {\n      this.resetStream_();\n      this.trigger('done', 'VideoSegmentStream');\n      return;\n    } // Organize the raw nal-units into arrays that represent\n    // higher-level constructs such as frames and gops\n    // (group-of-pictures)\n\n\n    frames = frameUtils.groupNalsIntoFrames(nalUnits);\n    gops = frameUtils.groupFramesIntoGops(frames); // If the first frame of this fragment is not a keyframe we have\n    // a problem since MSE (on Chrome) requires a leading keyframe.\n    //\n    // We have two approaches to repairing this situation:\n    // 1) GOP-FUSION:\n    //    This is where we keep track of the GOPS (group-of-pictures)\n    //    from previous fragments and attempt to find one that we can\n    //    prepend to the current fragment in order to create a valid\n    //    fragment.\n    // 2) KEYFRAME-PULLING:\n    //    Here we search for the first keyframe in the fragment and\n    //    throw away all the frames between the start of the fragment\n    //    and that keyframe. We then extend the duration and pull the\n    //    PTS of the keyframe forward so that it covers the time range\n    //    of the frames that were disposed of.\n    //\n    // #1 is far prefereable over #2 which can cause \"stuttering\" but\n    // requires more things to be just right.\n\n    if (!gops[0][0].keyFrame) {\n      // Search for a gop for fusion from our gopCache\n      gopForFusion = this.getGopForFusion_(nalUnits[0], track);\n\n      if (gopForFusion) {\n        // in order to provide more accurate timing information about the segment, save\n        // the number of seconds prepended to the original segment due to GOP fusion\n        prependedContentDuration = gopForFusion.duration;\n        gops.unshift(gopForFusion); // Adjust Gops' metadata to account for the inclusion of the\n        // new gop at the beginning\n\n        gops.byteLength += gopForFusion.byteLength;\n        gops.nalCount += gopForFusion.nalCount;\n        gops.pts = gopForFusion.pts;\n        gops.dts = gopForFusion.dts;\n        gops.duration += gopForFusion.duration;\n      } else {\n        // If we didn't find a candidate gop fall back to keyframe-pulling\n        gops = frameUtils.extendFirstKeyFrame(gops);\n      }\n    } // Trim gops to align with gopsToAlignWith\n\n\n    if (gopsToAlignWith.length) {\n      var alignedGops;\n\n      if (options.alignGopsAtEnd) {\n        alignedGops = this.alignGopsAtEnd_(gops);\n      } else {\n        alignedGops = this.alignGopsAtStart_(gops);\n      }\n\n      if (!alignedGops) {\n        // save all the nals in the last GOP into the gop cache\n        this.gopCache_.unshift({\n          gop: gops.pop(),\n          pps: track.pps,\n          sps: track.sps\n        }); // Keep a maximum of 6 GOPs in the cache\n\n        this.gopCache_.length = Math.min(6, this.gopCache_.length); // Clear nalUnits\n\n        nalUnits = []; // return early no gops can be aligned with desired gopsToAlignWith\n\n        this.resetStream_();\n        this.trigger('done', 'VideoSegmentStream');\n        return;\n      } // Some gops were trimmed. clear dts info so minSegmentDts and pts are correct\n      // when recalculated before sending off to CoalesceStream\n\n\n      trackDecodeInfo.clearDtsInfo(track);\n      gops = alignedGops;\n    }\n\n    trackDecodeInfo.collectDtsInfo(track, gops); // First, we have to build the index from byte locations to\n    // samples (that is, frames) in the video data\n\n    track.samples = frameUtils.generateSampleTable(gops); // Concatenate the video data and construct the mdat\n\n    mdat = mp4.mdat(frameUtils.concatenateNalData(gops));\n    track.baseMediaDecodeTime = trackDecodeInfo.calculateTrackBaseMediaDecodeTime(track, options.keepOriginalTimestamps);\n    this.trigger('processedGopsInfo', gops.map(function (gop) {\n      return {\n        pts: gop.pts,\n        dts: gop.dts,\n        byteLength: gop.byteLength\n      };\n    }));\n    firstGop = gops[0];\n    lastGop = gops[gops.length - 1];\n    this.trigger('segmentTimingInfo', generateVideoSegmentTimingInfo(track.baseMediaDecodeTime, firstGop.dts, firstGop.pts, lastGop.dts + lastGop.duration, lastGop.pts + lastGop.duration, prependedContentDuration)); // save all the nals in the last GOP into the gop cache\n\n    this.gopCache_.unshift({\n      gop: gops.pop(),\n      pps: track.pps,\n      sps: track.sps\n    }); // Keep a maximum of 6 GOPs in the cache\n\n    this.gopCache_.length = Math.min(6, this.gopCache_.length); // Clear nalUnits\n\n    nalUnits = [];\n    this.trigger('baseMediaDecodeTime', track.baseMediaDecodeTime);\n    this.trigger('timelineStartInfo', track.timelineStartInfo);\n    moof = mp4.moof(sequenceNumber, [track]); // it would be great to allocate this array up front instead of\n    // throwing away hundreds of media segment fragments\n\n    boxes = new Uint8Array(moof.byteLength + mdat.byteLength); // Bump the sequence number for next time\n\n    sequenceNumber++;\n    boxes.set(moof);\n    boxes.set(mdat, moof.byteLength);\n    this.trigger('data', {\n      track: track,\n      boxes: boxes\n    });\n    this.resetStream_(); // Continue with the flush process now\n\n    this.trigger('done', 'VideoSegmentStream');\n  };\n\n  this.resetStream_ = function () {\n    trackDecodeInfo.clearDtsInfo(track); // reset config and pps because they may differ across segments\n    // for instance, when we are rendition switching\n\n    config = undefined;\n    pps = undefined;\n  }; // Search for a candidate Gop for gop-fusion from the gop cache and\n  // return it or return null if no good candidate was found\n\n\n  this.getGopForFusion_ = function (nalUnit) {\n    var halfSecond = 45000,\n        // Half-a-second in a 90khz clock\n    allowableOverlap = 10000,\n        // About 3 frames @ 30fps\n    nearestDistance = Infinity,\n        dtsDistance,\n        nearestGopObj,\n        currentGop,\n        currentGopObj,\n        i; // Search for the GOP nearest to the beginning of this nal unit\n\n    for (i = 0; i < this.gopCache_.length; i++) {\n      currentGopObj = this.gopCache_[i];\n      currentGop = currentGopObj.gop; // Reject Gops with different SPS or PPS\n\n      if (!(track.pps && arrayEquals(track.pps[0], currentGopObj.pps[0])) || !(track.sps && arrayEquals(track.sps[0], currentGopObj.sps[0]))) {\n        continue;\n      } // Reject Gops that would require a negative baseMediaDecodeTime\n\n\n      if (currentGop.dts < track.timelineStartInfo.dts) {\n        continue;\n      } // The distance between the end of the gop and the start of the nalUnit\n\n\n      dtsDistance = nalUnit.dts - currentGop.dts - currentGop.duration; // Only consider GOPS that start before the nal unit and end within\n      // a half-second of the nal unit\n\n      if (dtsDistance >= -allowableOverlap && dtsDistance <= halfSecond) {\n        // Always use the closest GOP we found if there is more than\n        // one candidate\n        if (!nearestGopObj || nearestDistance > dtsDistance) {\n          nearestGopObj = currentGopObj;\n          nearestDistance = dtsDistance;\n        }\n      }\n    }\n\n    if (nearestGopObj) {\n      return nearestGopObj.gop;\n    }\n\n    return null;\n  }; // trim gop list to the first gop found that has a matching pts with a gop in the list\n  // of gopsToAlignWith starting from the START of the list\n\n\n  this.alignGopsAtStart_ = function (gops) {\n    var alignIndex, gopIndex, align, gop, byteLength, nalCount, duration, alignedGops;\n    byteLength = gops.byteLength;\n    nalCount = gops.nalCount;\n    duration = gops.duration;\n    alignIndex = gopIndex = 0;\n\n    while (alignIndex < gopsToAlignWith.length && gopIndex < gops.length) {\n      align = gopsToAlignWith[alignIndex];\n      gop = gops[gopIndex];\n\n      if (align.pts === gop.pts) {\n        break;\n      }\n\n      if (gop.pts > align.pts) {\n        // this current gop starts after the current gop we want to align on, so increment\n        // align index\n        alignIndex++;\n        continue;\n      } // current gop starts before the current gop we want to align on. so increment gop\n      // index\n\n\n      gopIndex++;\n      byteLength -= gop.byteLength;\n      nalCount -= gop.nalCount;\n      duration -= gop.duration;\n    }\n\n    if (gopIndex === 0) {\n      // no gops to trim\n      return gops;\n    }\n\n    if (gopIndex === gops.length) {\n      // all gops trimmed, skip appending all gops\n      return null;\n    }\n\n    alignedGops = gops.slice(gopIndex);\n    alignedGops.byteLength = byteLength;\n    alignedGops.duration = duration;\n    alignedGops.nalCount = nalCount;\n    alignedGops.pts = alignedGops[0].pts;\n    alignedGops.dts = alignedGops[0].dts;\n    return alignedGops;\n  }; // trim gop list to the first gop found that has a matching pts with a gop in the list\n  // of gopsToAlignWith starting from the END of the list\n\n\n  this.alignGopsAtEnd_ = function (gops) {\n    var alignIndex, gopIndex, align, gop, alignEndIndex, matchFound;\n    alignIndex = gopsToAlignWith.length - 1;\n    gopIndex = gops.length - 1;\n    alignEndIndex = null;\n    matchFound = false;\n\n    while (alignIndex >= 0 && gopIndex >= 0) {\n      align = gopsToAlignWith[alignIndex];\n      gop = gops[gopIndex];\n\n      if (align.pts === gop.pts) {\n        matchFound = true;\n        break;\n      }\n\n      if (align.pts > gop.pts) {\n        alignIndex--;\n        continue;\n      }\n\n      if (alignIndex === gopsToAlignWith.length - 1) {\n        // gop.pts is greater than the last alignment candidate. If no match is found\n        // by the end of this loop, we still want to append gops that come after this\n        // point\n        alignEndIndex = gopIndex;\n      }\n\n      gopIndex--;\n    }\n\n    if (!matchFound && alignEndIndex === null) {\n      return null;\n    }\n\n    var trimIndex;\n\n    if (matchFound) {\n      trimIndex = gopIndex;\n    } else {\n      trimIndex = alignEndIndex;\n    }\n\n    if (trimIndex === 0) {\n      return gops;\n    }\n\n    var alignedGops = gops.slice(trimIndex);\n    var metadata = alignedGops.reduce(function (total, gop) {\n      total.byteLength += gop.byteLength;\n      total.duration += gop.duration;\n      total.nalCount += gop.nalCount;\n      return total;\n    }, {\n      byteLength: 0,\n      duration: 0,\n      nalCount: 0\n    });\n    alignedGops.byteLength = metadata.byteLength;\n    alignedGops.duration = metadata.duration;\n    alignedGops.nalCount = metadata.nalCount;\n    alignedGops.pts = alignedGops[0].pts;\n    alignedGops.dts = alignedGops[0].dts;\n    return alignedGops;\n  };\n\n  this.alignGopsWith = function (newGopsToAlignWith) {\n    gopsToAlignWith = newGopsToAlignWith;\n  };\n};\n\n_VideoSegmentStream.prototype = new Stream();\n/**\n * A Stream that can combine multiple streams (ie. audio & video)\n * into a single output segment for MSE. Also supports audio-only\n * and video-only streams.\n * @param options {object} transmuxer options object\n * @param options.keepOriginalTimestamps {boolean} If true, keep the timestamps\n *        in the source; false to adjust the first segment to start at media timeline start.\n */\n\n_CoalesceStream = function CoalesceStream(options, metadataStream) {\n  // Number of Tracks per output segment\n  // If greater than 1, we combine multiple\n  // tracks into a single segment\n  this.numberOfTracks = 0;\n  this.metadataStream = metadataStream;\n  options = options || {};\n\n  if (typeof options.remux !== 'undefined') {\n    this.remuxTracks = !!options.remux;\n  } else {\n    this.remuxTracks = true;\n  }\n\n  if (typeof options.keepOriginalTimestamps === 'boolean') {\n    this.keepOriginalTimestamps = options.keepOriginalTimestamps;\n  }\n\n  this.pendingTracks = [];\n  this.videoTrack = null;\n  this.pendingBoxes = [];\n  this.pendingCaptions = [];\n  this.pendingMetadata = [];\n  this.pendingBytes = 0;\n  this.emittedTracks = 0;\n\n  _CoalesceStream.prototype.init.call(this); // Take output from multiple\n\n\n  this.push = function (output) {\n    // buffer incoming captions until the associated video segment\n    // finishes\n    if (output.text) {\n      return this.pendingCaptions.push(output);\n    } // buffer incoming id3 tags until the final flush\n\n\n    if (output.frames) {\n      return this.pendingMetadata.push(output);\n    } // Add this track to the list of pending tracks and store\n    // important information required for the construction of\n    // the final segment\n\n\n    this.pendingTracks.push(output.track);\n    this.pendingBoxes.push(output.boxes);\n    this.pendingBytes += output.boxes.byteLength;\n\n    if (output.track.type === 'video') {\n      this.videoTrack = output.track;\n    }\n\n    if (output.track.type === 'audio') {\n      this.audioTrack = output.track;\n    }\n  };\n};\n\n_CoalesceStream.prototype = new Stream();\n\n_CoalesceStream.prototype.flush = function (flushSource) {\n  var offset = 0,\n      event = {\n    captions: [],\n    captionStreams: {},\n    metadata: [],\n    info: {}\n  },\n      caption,\n      id3,\n      initSegment,\n      timelineStartPts = 0,\n      i;\n\n  if (this.pendingTracks.length < this.numberOfTracks) {\n    if (flushSource !== 'VideoSegmentStream' && flushSource !== 'AudioSegmentStream') {\n      // Return because we haven't received a flush from a data-generating\n      // portion of the segment (meaning that we have only recieved meta-data\n      // or captions.)\n      return;\n    } else if (this.remuxTracks) {\n      // Return until we have enough tracks from the pipeline to remux (if we\n      // are remuxing audio and video into a single MP4)\n      return;\n    } else if (this.pendingTracks.length === 0) {\n      // In the case where we receive a flush without any data having been\n      // received we consider it an emitted track for the purposes of coalescing\n      // `done` events.\n      // We do this for the case where there is an audio and video track in the\n      // segment but no audio data. (seen in several playlists with alternate\n      // audio tracks and no audio present in the main TS segments.)\n      this.emittedTracks++;\n\n      if (this.emittedTracks >= this.numberOfTracks) {\n        this.trigger('done');\n        this.emittedTracks = 0;\n      }\n\n      return;\n    }\n  }\n\n  if (this.videoTrack) {\n    timelineStartPts = this.videoTrack.timelineStartInfo.pts;\n    VIDEO_PROPERTIES.forEach(function (prop) {\n      event.info[prop] = this.videoTrack[prop];\n    }, this);\n  } else if (this.audioTrack) {\n    timelineStartPts = this.audioTrack.timelineStartInfo.pts;\n    AUDIO_PROPERTIES.forEach(function (prop) {\n      event.info[prop] = this.audioTrack[prop];\n    }, this);\n  }\n\n  if (this.pendingTracks.length === 1) {\n    event.type = this.pendingTracks[0].type;\n  } else {\n    event.type = 'combined';\n  }\n\n  this.emittedTracks += this.pendingTracks.length;\n  initSegment = mp4.initSegment(this.pendingTracks); // Create a new typed array to hold the init segment\n\n  event.initSegment = new Uint8Array(initSegment.byteLength); // Create an init segment containing a moov\n  // and track definitions\n\n  event.initSegment.set(initSegment); // Create a new typed array to hold the moof+mdats\n\n  event.data = new Uint8Array(this.pendingBytes); // Append each moof+mdat (one per track) together\n\n  for (i = 0; i < this.pendingBoxes.length; i++) {\n    event.data.set(this.pendingBoxes[i], offset);\n    offset += this.pendingBoxes[i].byteLength;\n  } // Translate caption PTS times into second offsets to match the\n  // video timeline for the segment, and add track info\n\n\n  for (i = 0; i < this.pendingCaptions.length; i++) {\n    caption = this.pendingCaptions[i];\n    caption.startTime = caption.startPts;\n\n    if (!this.keepOriginalTimestamps) {\n      caption.startTime -= timelineStartPts;\n    }\n\n    caption.startTime /= 90e3;\n    caption.endTime = caption.endPts;\n\n    if (!this.keepOriginalTimestamps) {\n      caption.endTime -= timelineStartPts;\n    }\n\n    caption.endTime /= 90e3;\n    event.captionStreams[caption.stream] = true;\n    event.captions.push(caption);\n  } // Translate ID3 frame PTS times into second offsets to match the\n  // video timeline for the segment\n\n\n  for (i = 0; i < this.pendingMetadata.length; i++) {\n    id3 = this.pendingMetadata[i];\n    id3.cueTime = id3.pts;\n\n    if (!this.keepOriginalTimestamps) {\n      id3.cueTime -= timelineStartPts;\n    }\n\n    id3.cueTime /= 90e3;\n    event.metadata.push(id3);\n  } // We add this to every single emitted segment even though we only need\n  // it for the first\n\n\n  event.metadata.dispatchType = this.metadataStream.dispatchType; // Reset stream state\n\n  this.pendingTracks.length = 0;\n  this.videoTrack = null;\n  this.pendingBoxes.length = 0;\n  this.pendingCaptions.length = 0;\n  this.pendingBytes = 0;\n  this.pendingMetadata.length = 0; // Emit the built segment\n\n  this.trigger('data', event); // Only emit `done` if all tracks have been flushed and emitted\n\n  if (this.emittedTracks >= this.numberOfTracks) {\n    this.trigger('done');\n    this.emittedTracks = 0;\n  }\n};\n/**\n * A Stream that expects MP2T binary data as input and produces\n * corresponding media segments, suitable for use with Media Source\n * Extension (MSE) implementations that support the ISO BMFF byte\n * stream format, like Chrome.\n */\n\n\n_Transmuxer = function Transmuxer(options) {\n  var self = this,\n      hasFlushed = true,\n      videoTrack,\n      audioTrack;\n\n  _Transmuxer.prototype.init.call(this);\n\n  options = options || {};\n  this.baseMediaDecodeTime = options.baseMediaDecodeTime || 0;\n  this.transmuxPipeline_ = {};\n\n  this.setupAacPipeline = function () {\n    var pipeline = {};\n    this.transmuxPipeline_ = pipeline;\n    pipeline.type = 'aac';\n    pipeline.metadataStream = new m2ts.MetadataStream(); // set up the parsing pipeline\n\n    pipeline.aacStream = new AacStream();\n    pipeline.audioTimestampRolloverStream = new m2ts.TimestampRolloverStream('audio');\n    pipeline.timedMetadataTimestampRolloverStream = new m2ts.TimestampRolloverStream('timed-metadata');\n    pipeline.adtsStream = new AdtsStream();\n    pipeline.coalesceStream = new _CoalesceStream(options, pipeline.metadataStream);\n    pipeline.headOfPipeline = pipeline.aacStream;\n    pipeline.aacStream.pipe(pipeline.audioTimestampRolloverStream).pipe(pipeline.adtsStream);\n    pipeline.aacStream.pipe(pipeline.timedMetadataTimestampRolloverStream).pipe(pipeline.metadataStream).pipe(pipeline.coalesceStream);\n    pipeline.metadataStream.on('timestamp', function (frame) {\n      pipeline.aacStream.setTimestamp(frame.timeStamp);\n    });\n    pipeline.aacStream.on('data', function (data) {\n      if (data.type === 'timed-metadata' && !pipeline.audioSegmentStream) {\n        audioTrack = audioTrack || {\n          timelineStartInfo: {\n            baseMediaDecodeTime: self.baseMediaDecodeTime\n          },\n          codec: 'adts',\n          type: 'audio'\n        }; // hook up the audio segment stream to the first track with aac data\n\n        pipeline.coalesceStream.numberOfTracks++;\n        pipeline.audioSegmentStream = new _AudioSegmentStream(audioTrack, options); // Set up the final part of the audio pipeline\n\n        pipeline.adtsStream.pipe(pipeline.audioSegmentStream).pipe(pipeline.coalesceStream);\n      }\n    }); // Re-emit any data coming from the coalesce stream to the outside world\n\n    pipeline.coalesceStream.on('data', this.trigger.bind(this, 'data')); // Let the consumer know we have finished flushing the entire pipeline\n\n    pipeline.coalesceStream.on('done', this.trigger.bind(this, 'done'));\n  };\n\n  this.setupTsPipeline = function () {\n    var pipeline = {};\n    this.transmuxPipeline_ = pipeline;\n    pipeline.type = 'ts';\n    pipeline.metadataStream = new m2ts.MetadataStream(); // set up the parsing pipeline\n\n    pipeline.packetStream = new m2ts.TransportPacketStream();\n    pipeline.parseStream = new m2ts.TransportParseStream();\n    pipeline.elementaryStream = new m2ts.ElementaryStream();\n    pipeline.videoTimestampRolloverStream = new m2ts.TimestampRolloverStream('video');\n    pipeline.audioTimestampRolloverStream = new m2ts.TimestampRolloverStream('audio');\n    pipeline.timedMetadataTimestampRolloverStream = new m2ts.TimestampRolloverStream('timed-metadata');\n    pipeline.adtsStream = new AdtsStream();\n    pipeline.h264Stream = new H264Stream();\n    pipeline.captionStream = new m2ts.CaptionStream();\n    pipeline.coalesceStream = new _CoalesceStream(options, pipeline.metadataStream);\n    pipeline.headOfPipeline = pipeline.packetStream; // disassemble MPEG2-TS packets into elementary streams\n\n    pipeline.packetStream.pipe(pipeline.parseStream).pipe(pipeline.elementaryStream); // !!THIS ORDER IS IMPORTANT!!\n    // demux the streams\n\n    pipeline.elementaryStream.pipe(pipeline.videoTimestampRolloverStream).pipe(pipeline.h264Stream);\n    pipeline.elementaryStream.pipe(pipeline.audioTimestampRolloverStream).pipe(pipeline.adtsStream);\n    pipeline.elementaryStream.pipe(pipeline.timedMetadataTimestampRolloverStream).pipe(pipeline.metadataStream).pipe(pipeline.coalesceStream); // Hook up CEA-608/708 caption stream\n\n    pipeline.h264Stream.pipe(pipeline.captionStream).pipe(pipeline.coalesceStream);\n    pipeline.elementaryStream.on('data', function (data) {\n      var i;\n\n      if (data.type === 'metadata') {\n        i = data.tracks.length; // scan the tracks listed in the metadata\n\n        while (i--) {\n          if (!videoTrack && data.tracks[i].type === 'video') {\n            videoTrack = data.tracks[i];\n            videoTrack.timelineStartInfo.baseMediaDecodeTime = self.baseMediaDecodeTime;\n          } else if (!audioTrack && data.tracks[i].type === 'audio') {\n            audioTrack = data.tracks[i];\n            audioTrack.timelineStartInfo.baseMediaDecodeTime = self.baseMediaDecodeTime;\n          }\n        } // hook up the video segment stream to the first track with h264 data\n\n\n        if (videoTrack && !pipeline.videoSegmentStream) {\n          pipeline.coalesceStream.numberOfTracks++;\n          pipeline.videoSegmentStream = new _VideoSegmentStream(videoTrack, options);\n          pipeline.videoSegmentStream.on('timelineStartInfo', function (timelineStartInfo) {\n            // When video emits timelineStartInfo data after a flush, we forward that\n            // info to the AudioSegmentStream, if it exists, because video timeline\n            // data takes precedence.\n            if (audioTrack) {\n              audioTrack.timelineStartInfo = timelineStartInfo; // On the first segment we trim AAC frames that exist before the\n              // very earliest DTS we have seen in video because Chrome will\n              // interpret any video track with a baseMediaDecodeTime that is\n              // non-zero as a gap.\n\n              pipeline.audioSegmentStream.setEarliestDts(timelineStartInfo.dts);\n            }\n          });\n          pipeline.videoSegmentStream.on('processedGopsInfo', self.trigger.bind(self, 'gopInfo'));\n          pipeline.videoSegmentStream.on('segmentTimingInfo', self.trigger.bind(self, 'videoSegmentTimingInfo'));\n          pipeline.videoSegmentStream.on('baseMediaDecodeTime', function (baseMediaDecodeTime) {\n            if (audioTrack) {\n              pipeline.audioSegmentStream.setVideoBaseMediaDecodeTime(baseMediaDecodeTime);\n            }\n          }); // Set up the final part of the video pipeline\n\n          pipeline.h264Stream.pipe(pipeline.videoSegmentStream).pipe(pipeline.coalesceStream);\n        }\n\n        if (audioTrack && !pipeline.audioSegmentStream) {\n          // hook up the audio segment stream to the first track with aac data\n          pipeline.coalesceStream.numberOfTracks++;\n          pipeline.audioSegmentStream = new _AudioSegmentStream(audioTrack, options); // Set up the final part of the audio pipeline\n\n          pipeline.adtsStream.pipe(pipeline.audioSegmentStream).pipe(pipeline.coalesceStream);\n        }\n      }\n    }); // Re-emit any data coming from the coalesce stream to the outside world\n\n    pipeline.coalesceStream.on('data', this.trigger.bind(this, 'data')); // Let the consumer know we have finished flushing the entire pipeline\n\n    pipeline.coalesceStream.on('done', this.trigger.bind(this, 'done'));\n  }; // hook up the segment streams once track metadata is delivered\n\n\n  this.setBaseMediaDecodeTime = function (baseMediaDecodeTime) {\n    var pipeline = this.transmuxPipeline_;\n\n    if (!options.keepOriginalTimestamps) {\n      this.baseMediaDecodeTime = baseMediaDecodeTime;\n    }\n\n    if (audioTrack) {\n      audioTrack.timelineStartInfo.dts = undefined;\n      audioTrack.timelineStartInfo.pts = undefined;\n      trackDecodeInfo.clearDtsInfo(audioTrack);\n\n      if (!options.keepOriginalTimestamps) {\n        audioTrack.timelineStartInfo.baseMediaDecodeTime = baseMediaDecodeTime;\n      }\n\n      if (pipeline.audioTimestampRolloverStream) {\n        pipeline.audioTimestampRolloverStream.discontinuity();\n      }\n    }\n\n    if (videoTrack) {\n      if (pipeline.videoSegmentStream) {\n        pipeline.videoSegmentStream.gopCache_ = [];\n        pipeline.videoTimestampRolloverStream.discontinuity();\n      }\n\n      videoTrack.timelineStartInfo.dts = undefined;\n      videoTrack.timelineStartInfo.pts = undefined;\n      trackDecodeInfo.clearDtsInfo(videoTrack);\n      pipeline.captionStream.reset();\n\n      if (!options.keepOriginalTimestamps) {\n        videoTrack.timelineStartInfo.baseMediaDecodeTime = baseMediaDecodeTime;\n      }\n    }\n\n    if (pipeline.timedMetadataTimestampRolloverStream) {\n      pipeline.timedMetadataTimestampRolloverStream.discontinuity();\n    }\n  };\n\n  this.setAudioAppendStart = function (timestamp) {\n    if (audioTrack) {\n      this.transmuxPipeline_.audioSegmentStream.setAudioAppendStart(timestamp);\n    }\n  };\n\n  this.alignGopsWith = function (gopsToAlignWith) {\n    if (videoTrack && this.transmuxPipeline_.videoSegmentStream) {\n      this.transmuxPipeline_.videoSegmentStream.alignGopsWith(gopsToAlignWith);\n    }\n  }; // feed incoming data to the front of the parsing pipeline\n\n\n  this.push = function (data) {\n    if (hasFlushed) {\n      var isAac = isLikelyAacData(data);\n\n      if (isAac && this.transmuxPipeline_.type !== 'aac') {\n        this.setupAacPipeline();\n      } else if (!isAac && this.transmuxPipeline_.type !== 'ts') {\n        this.setupTsPipeline();\n      }\n\n      hasFlushed = false;\n    }\n\n    this.transmuxPipeline_.headOfPipeline.push(data);\n  }; // flush any buffered data\n\n\n  this.flush = function () {\n    hasFlushed = true; // Start at the top of the pipeline and flush all pending work\n\n    this.transmuxPipeline_.headOfPipeline.flush();\n  }; // Caption data has to be reset when seeking outside buffered range\n\n\n  this.resetCaptions = function () {\n    if (this.transmuxPipeline_.captionStream) {\n      this.transmuxPipeline_.captionStream.reset();\n    }\n  };\n};\n\n_Transmuxer.prototype = new Stream();\nmodule.exports = {\n  Transmuxer: _Transmuxer,\n  VideoSegmentStream: _VideoSegmentStream,\n  AudioSegmentStream: _AudioSegmentStream,\n  AUDIO_PROPERTIES: AUDIO_PROPERTIES,\n  VIDEO_PROPERTIES: VIDEO_PROPERTIES,\n  // exported for testing\n  generateVideoSegmentTimingInfo: generateVideoSegmentTimingInfo\n};","map":null,"metadata":{},"sourceType":"script"}